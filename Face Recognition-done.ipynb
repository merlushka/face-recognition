{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition using neural network features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you have to construct face recognizer based on features extracted from the neural network. The task consists of two parts: image classification and video classification. In the first one you should classify distinct images and in the second one you will deal with short video sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Flatten, Dense, Activation\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import cPickle as pickle\n",
    "from copy import copy\n",
    "from collections import Counter\n",
    "from get_data import download, load_dataset, load_faces\n",
    "from preprocessing import preprocess_images, preprocess_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def VGG_model(weight_path=None):\n",
    "    \n",
    "    input_shape = (224,224,3)\n",
    "    model=Sequential()\n",
    "    # Block 1\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu', padding='same', name='conv1_1', input_shape=input_shape))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu', padding='same', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='pool1'))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu', padding='same', name='conv2_1'))\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu', padding='same', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='pool2'))\n",
    "\n",
    "    # Block 3\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu', padding='same', name='conv3_1'))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu', padding='same', name='conv3_2'))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu', padding='same', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='pool3'))\n",
    "\n",
    "    # Block 4\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu', padding='same', name='conv4_1'))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu', padding='same', name='conv4_2'))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu', padding='same', name='conv4_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='pool4'))\n",
    "\n",
    "    # Block 5\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu', padding='same', name='conv5_1'))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu', padding='same', name='conv5_2'))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu', padding='same', name='conv5_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='pool5'))\n",
    "\n",
    "    model.add(Flatten(name='flatten'))\n",
    "    model.add(Dense(4096, name='fc6'))\n",
    "    model.add(Activation('relu', name='fc6/relu'))\n",
    "    model.add(Dense(4096, name='fc7'))\n",
    "    model.add(Activation('relu', name='fc7/relu'))\n",
    "    model.add(Dense(500, name='fc8'))\n",
    "    model.add(Activation('relu', name='prob'))\n",
    "    if weight_path is not None:\n",
    "        if not os.path.exists(weight_path):\n",
    "            download(weight_path)\n",
    "        model.load_weights(weight_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you load the data for the first time it can take long time (especially for the deep network weights) as firstly the data will be downloaded from the Internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the data you will work with. All the images contain a face with some background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_dataset('images')\n",
    "classes = np.unique(y_train)\n",
    "print '%d'%len(x_train), '\\ttraining images'\n",
    "print '%d'%len(x_test), '\\ttesting images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "people_faces = load_faces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize(data, labels, function = lambda x:x, n_cols = 5, n_rows=1):\n",
    "    figure(figsize = (3*n_cols,3*n_rows))\n",
    "    for n,i in enumerate(np.random.randint(len(data), size = n_cols*n_rows)):\n",
    "        plt.subplot(n_rows,n_cols,n+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(function(data[i]))\n",
    "        plt.title(labels[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is how the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize(x_train,y_train)\n",
    "visualize(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to implement preprocessing function in the cell below.\n",
    "Getting an image as an input the this function should detect the face on it, find the facial keypoints and then crop and normalize the image \n",
    "according to these keypoints. The output image should contain only the aligned face and should be the tensor of the shape (1, 224, 224, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    \"\"\" Your implementation \"\"\" \n",
    "    return preprocess_images(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize(x_train,y_train, function = lambda x:preprocess(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network is already trained on the other face dataset. You should use this network as feature extractor to get descriptors of the faces. You can choose any hidden layer you need (or several layers) to extract features and any classification method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = VGG_model('vgg_face_500.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of using the network as feature extractor. The shape of input tensor has to be (n_images, 224, 224, 3), so you can input several images simultaneously and get their face descriptors of shape (n_images, n_components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_layer_output(images, layer = 'fc7'):\n",
    "    assert len(images.shape)==4, 'Wrong input dimentionality!'\n",
    "    assert images.shape[1:]==(224,224,3), 'Wrong input shape!'\n",
    "    \n",
    "    network_output = model.get_layer(layer).output\n",
    "    feature_extraction_model = Model(model.input, network_output)\n",
    "    \n",
    "    output = feature_extraction_model.predict(images)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.resize(x_train[0], (224,224)).reshape(1,224,224,3)\n",
    "out = get_layer_output(img)\n",
    "print out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to implement two functions in the cell below.\n",
    "The function \"classify\" should return the name of the most probable person shown on the image, and\n",
    "\"predict_proba\" should return the list of probabilities. Now these functions return random result, you should change it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all training images preprocessed\n",
    "train_out = preprocess(x_train[0])\n",
    "for im in x_train[1:]:\n",
    "    train_out = np.concatenate((train_out, preprocess(im)), axis = 0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "#neural features of all training images\n",
    "t0 = time.time()\n",
    "last = len(train_out)/100 + 1\n",
    "features = get_layer_output(train_out[:100])\n",
    "for i in range(1,last):\n",
    "    res = get_layer_output(train_out[100*i:100*i+100])\n",
    "    features = np.concatenate((features, res), axis = 0)\n",
    "    print features.shape\n",
    "print time.time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kNN = KNeighborsClassifier(n_neighbors=1)\n",
    "kNN.fit(features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all testing images preprocessed\n",
    "test_out = preprocess(x_test[0])\n",
    "for im in x_test[1:]:\n",
    "    test_out = np.concatenate((test_out, preprocess(im)), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#neural features of all testing images\n",
    "t0 = time.time()\n",
    "last = len(test_out)/100 + 1\n",
    "test_features = get_layer_output(test_out[:100])\n",
    "for i in range(1,last):\n",
    "    res = get_layer_output(test_out[100*i:100*i+100])\n",
    "    test_features = np.concatenate((test_features, res), axis = 0)\n",
    "    print test_features.shape\n",
    "print time.time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(features):\n",
    "    \n",
    "    return kNN.predict([features])\n",
    "    \n",
    "def predict_proba(img, img_id=0):\n",
    "    \n",
    "    feature = get_layer_output(preprocess(img))\n",
    "    return kNN.predict_proba(feature)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the accuracy of your classification. Sometimes it is more convenient to classify the block of images simultaneously, so you can change this script if you need. But you have to get the list of the predictions for each of the testing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "for features in test_features:\n",
    "    label = classify(features)\n",
    "    labels.append(label)\n",
    "pickle.dump(labels, open('result_images.pickle', 'wb'))\n",
    "print 'Classification accuracy:\\t%3f' % accuracy_score(labels, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Visualization of the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_5_visualization(test_data, test_labels, classes,prediction_function, is_video = False, n_images = 3):\n",
    "    figure(figsize = (18,10))\n",
    "    for n,i in enumerate(np.random.randint(len(test_data), size = n_images)):\n",
    "        plt.subplot(n_images,6,6*n+1)\n",
    "        plt.axis('off')\n",
    "        if is_video:\n",
    "            plt.imshow(test_data[i][0])\n",
    "        else:\n",
    "            plt.imshow(test_data[i])\n",
    "        plt.title('Request')\n",
    "        preds = prediction_function(test_data[i],i)\n",
    "        labels = preds.argsort()[-1:-6:-1]\n",
    "\n",
    "        for j,l in enumerate(labels):\n",
    "            plt.subplot(n_images,6, 6*n+j+2)\n",
    "            plt.axis('off')\n",
    "            picture = copy(people_faces[classes[l]])\n",
    "            plt.title('Top-%d'%(j+1))\n",
    "            if test_labels[i]==classes[l]:\n",
    "                cv2.rectangle(picture,(0,0),picture.shape[:2], (0,250,0),15)\n",
    "            plt.imshow(picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_5_visualization(x_test, y_test, classes,prediction_function=predict_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Face recognition in video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have to classify faces in video sequences. Each sequence containes about 125 frames with a face depicted on each frame. You should detect the face, find the keypoints and normalize the images as in the previous task (you can use the same preprocess function). To classify the whole video you can combine the predictions for its frames any way you want (averaging, voting, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data is in the same format as in the first task. There are distinct images with different faces depicted on them. Testing data is the dictionary: the keys are video ids and the values are lists of frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_train, train_labels, video_test, test_labels = load_dataset('video')\n",
    "video_classes = np.unique(train_labels)\n",
    "print '%d'%len(video_train), '\\ttraining images'\n",
    "print '%d'%len(video_test), '\\ttesting videos'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to implement two functions in the cell below.\n",
    "The function \"classify\" should return the name of the most probable person in video, and\n",
    "\"predict_proba\" should return the list of probabilities. Now these functions return random result, you should change it.\n",
    "\n",
    "Hint: while preprocessing video frames you can use face detector not in all the frames but every few frames and interpolate face detections in other frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all training images preprocessed\n",
    "v_train_out = preprocess(video_train[0])\n",
    "for im in video_train[1:]:\n",
    "    v_train_out = np.concatenate((v_train_out, preprocess(im)), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#neural features for all training images\n",
    "t0 = time.time()\n",
    "last = len(v_train_out)/100 + 1\n",
    "v_features = get_layer_output(v_train_out[:100])\n",
    "for i in range(1,last):\n",
    "    res = get_layer_output(v_train_out[100*i:100*i+100])\n",
    "    v_features = np.concatenate((v_features, res), axis = 0)\n",
    "    print v_features.shape\n",
    "print time.time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v_features = v_features/np.linalg.norm(v_features,axis=1,keepdims=True)\n",
    "neigh = KNeighborsClassifier(n_neighbors=1, p=2)\n",
    "neigh.fit(v_features, train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all frames of testing videos preprocessed\n",
    "with open('frame_bboxes.pickle','r') as f:\n",
    "    bboxes = pickle.load(f)\n",
    "    \n",
    "v_test_out = {}\n",
    "for v in video_test:\n",
    "    print v\n",
    "    v_out = preprocess_frames(video_test[v][0],v,0, bboxes)\n",
    "    for i in range(1,len(video_test[v])):\n",
    "        v_out = np.concatenate((v_out, preprocess_frames(video_test[v][i],v,i, bboxes)), axis = 0)\n",
    "    v_test_out[v] = v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#neural features of all the frames of all testing videos\n",
    "t0=time.time()\n",
    "video_descs = {}\n",
    "for v in v_test_out:\n",
    "    print v    \n",
    "    fit_data = v_test_out[v]\n",
    "    layer_outs = get_layer_output(fit_data)\n",
    "    video_descs[v] = layer_outs/np.linalg.norm(layer_outs, axis=1, keepdims=True)\n",
    "print time.time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_video(video_features):\n",
    "    \n",
    "    prob = neigh.predict(video_features)\n",
    "    return (Counter(prob).most_common(1)[0][0])    \n",
    "    \n",
    "def predict_proba_video(video,video_id):\n",
    "    v_out = preprocess_frames(video[0],video_id,0, bboxes)\n",
    "    for i in range(1,len(video)):\n",
    "        v_out = np.concatenate((v_out, preprocess_frames(video[i],video_id,i, bboxes)), axis = 0)\n",
    "    v_desc = get_layer_output(v_out)    \n",
    "    prob = neigh.predict_proba(v_desc)\n",
    "    \n",
    "    voting = np.argmax(prob, axis = 1)\n",
    "    summ = len(prob)\n",
    "    probabilities = np.zeros(len(video_classes))\n",
    "    for k in Counter(voting):\n",
    "        probabilities[k]=Counter(voting)[k]/float(summ)\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_labels = []\n",
    "for video_id in video_descs:  \n",
    "    label = classify_video(video_descs[video_id])\n",
    "    video_labels.append(label)\n",
    "pickle.dump(video_labels, open('result_video.pickle', 'wb'))\n",
    "\n",
    "print 'Classification accuracy:\\t%3f' % accuracy_score(video_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_5_visualization(video_test, test_labels, video_classes, predict_proba_video, is_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
